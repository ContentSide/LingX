{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALAPP_2021_Paper.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNdPrjzezI_n"
      },
      "source": [
        "#Install and Config LingX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbVhjztiYmIR"
      },
      "source": [
        "!pip install lingx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnCEO8BwzQrZ"
      },
      "source": [
        "#Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC5czLQljHVA"
      },
      "source": [
        "import lingx.utils.download_lang_models\n",
        "from lingx.core.lang_model import get_nlp_object\n",
        "\n",
        "nlp_en = get_nlp_object(\"en\", use_critt_tokenization = True, package=\"partut\")\n",
        "nlp_zh = get_nlp_object(\"zh\", use_critt_tokenization = True)\n",
        "\n",
        "from lingx.utils.critt.aligner import generate_alignment_pipelines\n",
        "\n",
        "from lingx.utils.critt.tables import readTPRDBtables\n",
        "from lingx.utils.critt.tables import convert_st2segment , convert_tt2segment\n",
        "from lingx.utils.critt.tables import expand_table_psycholingual , expand_table_monolingual , expand_table_bilingual\n",
        "from lingx.utils.critt.tables import merge_st_tt , expand_table_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe3DKyw2T8ri"
      },
      "source": [
        "# Convert ST an TT to Segment DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_E-PH1NT8EB"
      },
      "source": [
        "!git clone https://github.com/ContentSide/lingx.git\n",
        "path_to_tprdb = \"/content/lingx/resources/TPRDB/EN-ZH_IMBst18/\"\n",
        "\n",
        "df_sg = readTPRDBtables(['Tables/'], \"*sg\", verbose=1, path=path_to_tprdb)\n",
        "df_st = readTPRDBtables(['Tables/'], \"*st\", verbose=1, path=path_to_tprdb)\n",
        "df_tt = readTPRDBtables(['Tables/'], \"*tt\", verbose=1, path=path_to_tprdb)\n",
        "\n",
        "alignments_offset = generate_alignment_pipelines(df_st, df_tt)\n",
        "\n",
        "analysis_st = convert_st2segment(df_st)\n",
        "analysis_tt = convert_tt2segment(df_tt)\n",
        "\n",
        "analysis_st"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW56vRvDzYpN"
      },
      "source": [
        "#Calculate the Complexities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6DTfW20abRt"
      },
      "source": [
        "import numpy\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "analysis_st = expand_table_psycholingual(analysis_st, nlp_en, token_column=\"SToken\")\n",
        "analysis_tt = expand_table_psycholingual(analysis_tt, nlp_zh, token_column=\"TToken\")\n",
        "\n",
        "analysis_st =  expand_table_monolingual(analysis_st, nlp_en, token_column=\"SToken\")\n",
        "analysis_tt =  expand_table_monolingual(analysis_tt, nlp_zh, token_column=\"TToken\")\n",
        "\n",
        "analysis_st_tt = merge_st_tt(analysis_st, analysis_tt, alignments_offset)\n",
        "analysis_st_tt = expand_table_bilingual(analysis_st_tt, nlp_en, nlp_zh, robust=True, bcr_error_value=numpy.nan)\n",
        "\n",
        "\n",
        "print(analysis_st_tt)\n",
        "print(analysis_st_tt.columns)\n",
        "\n",
        "print(\"Running Time (Min): \",round((time.time() - start_time)/60,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIVoEEU_zk0U"
      },
      "source": [
        "#Connect the Metric Results to Human-level Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ebQMuKRmpze"
      },
      "source": [
        "error_file_path = \"/content/lingx/resources/TPRDB/EN-ZH_IMBst18/HumanEvaluations/errors.csv\"\n",
        "analysis_st_tt = expand_table_error(analysis_st_tt, error_file_path)\n",
        "\n",
        "analysis_st_tt.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdhmYkqoWd9O"
      },
      "source": [
        "#Filter Table on Numeric Measures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNbMQgrHfi-_"
      },
      "source": [
        "table = analysis_st_tt[[\n",
        "                        'IDT_MAX_TT', 'IDT_MEAN_TT', 'IDT_SUM_TT', 'DLT_MAX_TT', 'DLT_MEAN_TT', \n",
        "                        'DLT_SUM_TT', 'IDT_DLT_MAX_TT', 'IDT_DLT_MEAN_TT', 'IDT_DLT_SUM_TT', 'LE_MEAN_TT',\n",
        "                        'LE_MAX_TT', 'LE_SUM_TT', 'MBN_MEAN_TT', 'MBN_MAX_TT', 'MBN_SUM_TT',\n",
        "                        'SToken', 'IDT_MAX_ST', 'IDT_MEAN_ST', 'IDT_SUM_ST', 'DLT_MAX_ST',\n",
        "                        'DLT_MEAN_ST', 'DLT_SUM_ST', 'IDT_DLT_MAX_ST', 'IDT_DLT_MEAN_ST',\n",
        "                        'IDT_DLT_SUM_ST', 'LE_MEAN_ST', 'LE_MAX_ST', 'LE_SUM_ST', 'MBN_MEAN_ST',\n",
        "                        'MBN_MAX_ST', 'MBN_SUM_ST', 'Alignment', 'BCR_SUM_SUM_SUM',\n",
        "                        'BCR_SUM_SUM_MAX', 'BCR_SUM_SUM_MEAN', 'BCR_SUM_MAX_SUM',\n",
        "                        'BCR_SUM_MAX_MAX', 'BCR_SUM_MAX_MEAN', 'BCR_SUM_MEAN_SUM',\n",
        "                        'BCR_SUM_MEAN_MAX', 'BCR_SUM_MEAN_MEAN', \n",
        "                        'Any', 'Accuracy', 'Fluency', 'Style', 'Critical', 'Minor'\n",
        "                        ]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d6PVsNDz0uY"
      },
      "source": [
        "#Correlation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi8SOvzVsOeZ"
      },
      "source": [
        "report = table.corr(method=\"spearman\")\n",
        "# report.to_csv(\"report.csv\")\n",
        "report[['Any','Accuracy','Fluency','Style','Critical','Minor']]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}